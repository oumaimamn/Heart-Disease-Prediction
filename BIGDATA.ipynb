{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Initialiser Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse des maladies cardiaques\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Collecte de Données\n",
    "# Charger le fichier CSV (le fichier uploadé dans Colab)\n",
    "file_path = \"heart.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "# Aperçu des données\n",
    "data.show(5)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Préparation des Données\n",
    "# a) Gérer les valeurs manquantes\n",
    "data = data.na.fill(0)  # Remplacer les valeurs nulles par 0\n",
    "\n",
    "# b) Supprimer les doublons\n",
    "data = data.dropDuplicates()\n",
    "\n",
    "# c) Renommer les colonnes pour une meilleure lisibilité\n",
    "for col_name in data.columns:\n",
    "    data = data.withColumnRenamed(col_name, col_name.lower())\n",
    "\n",
    "# Valider les changements\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Stockage des Données\n",
    "# Sauvegarder les données nettoyées dans un fichier Parquet (simulation de HDFS)\n",
    "data.write.parquet(\"/content/cleaned_data.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analyse Exploratoire des Données (EDA)\n",
    "# a) Utiliser Spark SQL\n",
    "# Créer une vue temporaire pour exécuter des requêtes SQL\n",
    "data.createOrReplaceTempView(\"heart_data\")\n",
    "\n",
    "# Exemple : compter les entrées par groupe \"target\"\n",
    "target_count = spark.sql(\"SELECT target, COUNT(*) as count FROM heart_data GROUP BY target\")\n",
    "target_count.show()\n",
    "\n",
    "# b) Visualisation avec Matplotlib et Seaborn\n",
    "# Convertir les données Spark en Pandas pour la visualisation\n",
    "pandas_df = target_count.toPandas()\n",
    "\n",
    "# Visualisation\n",
    "sns.barplot(data=pandas_df, x=\"target\", y=\"count\")\n",
    "plt.title(\"Distribution des cibles (target)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Modélisation et Analyse\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# a) Préparation des données pour le clustering\n",
    "feature_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']  # Exemple de colonnes\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "cluster_data = assembler.transform(data)\n",
    "\n",
    "# b) Appliquer l'algorithme de clustering KMeans\n",
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\")\n",
    "model = kmeans.fit(cluster_data)\n",
    "clusters = model.transform(cluster_data)\n",
    "\n",
    "# c) Afficher les résultats du clustering\n",
    "clusters.select(\"age\", \"trestbps\", \"chol\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualisation des Résultats\n",
    "# Exemple de visualisation des clusters\n",
    "cluster_pandas = clusters.select(\"age\", \"trestbps\", \"chol\", \"prediction\").toPandas()\n",
    "sns.scatterplot(data=cluster_pandas, x=\"age\", y=\"chol\", hue=\"prediction\", palette=\"viridis\")\n",
    "plt.title(\"Clusters basés sur l'âge et le cholestérol\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.select(\"age\", \"trestbps\", \"chol\", \"prediction\").write.csv(\"/content/heart.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
